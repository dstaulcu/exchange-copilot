# Exchange MCP Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Data Storage Paths
# =============================================================================
# Base directory for all data files (relative to project root or absolute)
DATA_PATH=data

# JSON data cache file (mock mode)
# Can be absolute or relative to DATA_PATH
DATA_FILE=data/exchange_mcp.json

# ChromaDB vector store path
# Can be absolute or relative to DATA_PATH
CHROMA_DB_PATH=data/chroma_db

# =============================================================================
# Data Source Configuration
# =============================================================================
# Options: mock, graph, ews
# - mock: Read from local JSON file (default, for testing)
# - graph: Connect to Microsoft 365 via Microsoft Graph API
# - ews: Connect to Exchange Server via Exchange Web Services
DATA_SOURCE=mock

# --- Mock Mode (DATA_SOURCE=mock) ---
# Uses DATA_FILE path configured above

# --- Microsoft Graph Mode (DATA_SOURCE=graph) ---
# Register an app at https://portal.azure.com
# Required permissions: User.Read, User.Read.All, Mail.Read, Calendars.Read
# GRAPH_TENANT_ID=your-tenant-id
# GRAPH_CLIENT_ID=your-app-client-id
# GRAPH_CLIENT_SECRET=your-client-secret
# GRAPH_USER_EMAIL=user@company.com

# --- Exchange Web Services Mode (DATA_SOURCE=ews) ---
# For on-premises Exchange or Exchange Online via EWS
# EWS_SERVER=mail.company.com
# EWS_EMAIL=user@company.com
# EWS_USERNAME=DOMAIN\\username
# EWS_PASSWORD=your-password
# EWS_AUTODISCOVER=true

# =============================================================================
# Backend Server
# =============================================================================
HOST=127.0.0.1
PORT=8000

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Options: "ollama" (local) or "openai" (OpenAI-compatible API)
LLM_PROVIDER=ollama

# Model name
# - For Ollama: llama3.2, llama3.2:3b, mistral, etc.
# - For OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo, etc.
# - For other providers: check their documentation
LLM_MODEL=llama3.2

# --- Ollama (LLM_PROVIDER=ollama) ---
# No additional config needed - just run `ollama serve`
# Legacy setting (still works): OLLAMA_MODEL=llama3.2

# --- OpenAI or OpenAI-Compatible APIs (LLM_PROVIDER=openai) ---
# Works with: OpenAI, Azure OpenAI, LM Studio, vLLM, Ollama (OpenAI mode), etc.
# LLM_API_KEY=sk-your-api-key
# LLM_BASE_URL=https://api.openai.com/v1
#
# Examples:
#   OpenAI:       LLM_BASE_URL=https://api.openai.com/v1
#   LM Studio:    LLM_BASE_URL=http://localhost:1234/v1
#   vLLM:         LLM_BASE_URL=http://localhost:8000/v1
#   Ollama:       LLM_BASE_URL=http://localhost:11434/v1
#   Together.ai:  LLM_BASE_URL=https://api.together.xyz/v1

# =============================================================================
# Sync & Debug
# =============================================================================
# Data Sync Interval (in minutes)
# How often to check for new emails/meetings
SYNC_INTERVAL_MINUTES=5

# Debug Mode
# Set to true for auto-reload and verbose logging
DEBUG=false
